{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 TRUE Cyclic Precision Training\n",
        "\n",
        "This notebook implements **TRUE Cyclic Precision Training (CPT)** for GPT-2, where a single model dynamically cycles through precision levels during training.\n",
        "\n",
        "## Project Overview:\n",
        "\n",
        "**TRUE Cyclic Precision Training** implements temporal precision cycling:\n",
        "- **Single model** that cycles through precision levels (16→8→4→8→16)\n",
        "- **Repeated cycles** over training steps (e.g., 5 cycles over 1000 steps)\n",
        "- **Real-time precision changes** during training without separate models\n",
        "- **Comprehensive comparison** with static precision baselines\n",
        "\n",
        "**Key Innovations:**\n",
        "1. **Temporal Precision Cycling**: Single model cycles through precision levels over time\n",
        "2. **Dynamic Quantization**: Real-time precision adjustment during training steps\n",
        "3. **True Cyclic Scheduling**: 16→8→4→8→16 pattern repeated multiple times\n",
        "4. **Performance Comparison**: Systematic evaluation against static approaches\n",
        "\n",
        "## Steps:\n",
        "1. Setup and imports\n",
        "2. True cyclic precision scheduling system\n",
        "3. Adaptive quantization with LoRA\n",
        "4. Cyclic precision trainer implementation\n",
        "5. Model setup and data loading\n",
        "6. Training with true cyclic precision\n",
        "7. Evaluation and comparison\n",
        "8. Results analysis and visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: C:\\Users\\Shriansh\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets evaluate peft torch bitsandbytes accelerate matplotlib pandas tqdm seaborn numpy -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import copy\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from typing import Dict, List, Optional, Union, Any, Callable\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    GPT2Config,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Disable wandb logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: TRUE Cyclic Precision Scheduling System\n",
        "\n",
        "Implement temporal precision cycling where a single model cycles through precision levels over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True cyclic precision scheduling system implemented!\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class PrecisionConfig:\n",
        "    \"\"\"Configuration for precision cycling.\"\"\"\n",
        "    min_bits: int = 4\n",
        "    max_bits: int = 16\n",
        "    total_steps: int = 1000\n",
        "    num_cycles: int = 5  # Number of complete cycles over total_steps\n",
        "    precision_sequence: List[int] = None  # Custom sequence like [16, 8, 4, 8, 16]\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        if self.precision_sequence is None:\n",
        "            # Default sequence: 16->8->4->8->16\n",
        "            self.precision_sequence = [16, 8, 4, 8, 16]\n",
        "        \n",
        "        # Calculate steps per precision level\n",
        "        self.steps_per_level = self.total_steps // (len(self.precision_sequence) * self.num_cycles)\n",
        "\n",
        "class TrueCyclicPrecisionScheduler:\n",
        "    \"\"\"True cyclic precision scheduler that cycles through precision levels multiple times.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrecisionConfig):\n",
        "        self.config = config\n",
        "        self.precision_sequence = config.precision_sequence\n",
        "        self.steps_per_level = config.steps_per_level\n",
        "        self.total_steps = config.total_steps\n",
        "        self.num_cycles = config.num_cycles\n",
        "        \n",
        "        print(f\"Initialized TrueCyclicPrecisionScheduler:\")\n",
        "        print(f\"  - Precision sequence: {self.precision_sequence}\")\n",
        "        print(f\"  - Steps per level: {self.steps_per_level}\")\n",
        "        print(f\"  - Total steps: {self.total_steps}\")\n",
        "        print(f\"  - Number of cycles: {self.num_cycles}\")\n",
        "    \n",
        "    def get_current_precision(self, step: int) -> int:\n",
        "        \"\"\"Get precision for current training step.\"\"\"\n",
        "        if step >= self.total_steps:\n",
        "            # If we exceed total steps, use the last precision in sequence\n",
        "            return self.precision_sequence[-1]\n",
        "        \n",
        "        # Calculate which cycle we're in\n",
        "        cycle_step = step % (len(self.precision_sequence) * self.steps_per_level)\n",
        "        \n",
        "        # Calculate which precision level we're in within the cycle\n",
        "        level_index = cycle_step // self.steps_per_level\n",
        "        \n",
        "        # Get the precision for this level\n",
        "        current_precision = self.precision_sequence[level_index]\n",
        "        \n",
        "        return current_precision\n",
        "    \n",
        "    def get_precision_schedule(self) -> List[tuple]:\n",
        "        \"\"\"Get the complete precision schedule for visualization.\"\"\"\n",
        "        schedule = []\n",
        "        for step in range(self.total_steps):\n",
        "            precision = self.get_current_precision(step)\n",
        "            schedule.append((step, precision))\n",
        "        return schedule\n",
        "\n",
        "def create_true_cyclic_scheduler(config: PrecisionConfig) -> TrueCyclicPrecisionScheduler:\n",
        "    \"\"\"Create a true cyclic precision scheduler.\"\"\"\n",
        "    return TrueCyclicPrecisionScheduler(config)\n",
        "\n",
        "print(\"True cyclic precision scheduling system implemented!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Adaptive Quantization with Cyclic Precision\n",
        "\n",
        "Implement dynamic quantization that responds to cyclic precision changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cyclic quantization and adaptive LoRA system implemented!\n"
          ]
        }
      ],
      "source": [
        "class CyclicQuantization:\n",
        "    \"\"\"Dynamic quantization system that adapts to cyclic precision changes.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.current_precision = 16  # Default to full precision\n",
        "        self.quantization_cache = {}  # Cache quantized weights\n",
        "        self.original_weights = {}    # Store original weights\n",
        "\n",
        "    def set_precision(self, precision: int):\n",
        "        \"\"\"Update current precision level.\"\"\"\n",
        "        self.current_precision = precision\n",
        "\n",
        "    def quantize_weight(self, weight: torch.Tensor, layer_name: str = None) -> torch.Tensor:\n",
        "        \"\"\"Quantize weight tensor based on current precision.\"\"\"\n",
        "        if self.current_precision == 16:\n",
        "            return weight.half()\n",
        "        elif self.current_precision == 8:\n",
        "            return self._quantize_int8(weight)\n",
        "        elif self.current_precision == 4:\n",
        "            return self._quantize_int4(weight)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported precision: {self.current_precision}\")\n",
        "\n",
        "    def _quantize_int8(self, weight: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"8-bit quantization.\"\"\"\n",
        "        scale = weight.abs().max() / 127.0\n",
        "        if scale == 0:\n",
        "            return weight\n",
        "        quantized = torch.round(weight / scale).clamp(-128, 127)\n",
        "        return (quantized * scale).to(weight.dtype)\n",
        "\n",
        "    def _quantize_int4(self, weight: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"4-bit quantization.\"\"\"\n",
        "        scale = weight.abs().max() / 7.0\n",
        "        if scale == 0:\n",
        "            return weight\n",
        "        quantized = torch.round(weight / scale).clamp(-8, 7)\n",
        "        return (quantized * scale).to(weight.dtype)\n",
        "\n",
        "class CyclicAdaptiveLoRAModule(nn.Module):\n",
        "    \"\"\"LoRA module that adapts to cyclic precision changes.\"\"\"\n",
        "\n",
        "    def __init__(self, base_layer, r=8, lora_alpha=32, lora_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.base_layer = base_layer\n",
        "        self.r = r\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.lora_dropout = lora_dropout\n",
        "        self.current_precision = 16\n",
        "\n",
        "        # Create LoRA components\n",
        "        in_features = base_layer.in_features\n",
        "        out_features = base_layer.out_features\n",
        "\n",
        "        self.lora_A = nn.Linear(in_features, r, bias=False)\n",
        "        self.lora_B = nn.Linear(r, out_features, bias=False)\n",
        "        self.dropout = nn.Dropout(lora_dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.kaiming_uniform_(self.lora_A.weight, a=np.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B.weight)\n",
        "\n",
        "        # Store original weights\n",
        "        self.original_weight = base_layer.weight.data.clone()\n",
        "\n",
        "    def update_precision(self, precision: int, quantizer: CyclicQuantization):\n",
        "        \"\"\"Update precision and re-quantize base layer weights.\"\"\"\n",
        "        if precision != self.current_precision:\n",
        "            self.current_precision = precision\n",
        "            \n",
        "            # Re-quantize base layer weights\n",
        "            with torch.no_grad():\n",
        "                quantized_weight = quantizer.quantize_weight(self.original_weight)\n",
        "                self.base_layer.weight.data = quantized_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        base_output = self.base_layer(x)\n",
        "        \n",
        "        # LoRA computation\n",
        "        lora_output = self.lora_A(self.dropout(x))\n",
        "        lora_output = self.lora_B(lora_output)\n",
        "        \n",
        "        scaling = self.lora_alpha / self.r\n",
        "        return base_output + lora_output * scaling\n",
        "\n",
        "class CyclicAdaptiveGPT2(nn.Module):\n",
        "    \"\"\"GPT-2 model with cyclic precision training and adaptive LoRA.\"\"\"\n",
        "\n",
        "    def __init__(self, base_model, quantizer: CyclicQuantization):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.quantizer = quantizer\n",
        "        self.adaptive_lora_layers = nn.ModuleDict()\n",
        "        self.current_precision = 16\n",
        "        self._replace_linear_layers()\n",
        "\n",
        "    def _replace_linear_layers(self):\n",
        "        \"\"\"Replace target linear layers with adaptive LoRA modules.\"\"\"\n",
        "        target_modules = ['c_attn', 'c_proj', 'c_fc']\n",
        "\n",
        "        for name, module in self.base_model.named_modules():\n",
        "            if isinstance(module, nn.Linear) and any(target in name for target in target_modules):\n",
        "                adaptive_lora = CyclicAdaptiveLoRAModule(module)\n",
        "                self.adaptive_lora_layers[name] = adaptive_lora\n",
        "\n",
        "                # Replace the module in the base model\n",
        "                self._replace_module_in_model(name, adaptive_lora)\n",
        "\n",
        "    def _replace_module_in_model(self, module_name: str, new_module):\n",
        "        \"\"\"Replace a module in the model hierarchy.\"\"\"\n",
        "        parts = module_name.split('.')\n",
        "        parent = self.base_model\n",
        "        \n",
        "        # Navigate to parent module\n",
        "        for part in parts[:-1]:\n",
        "            parent = getattr(parent, part)\n",
        "        \n",
        "        # Replace the target module\n",
        "        setattr(parent, parts[-1], new_module)\n",
        "\n",
        "    def update_precision(self, precision: int):\n",
        "        \"\"\"Update precision for all adaptive LoRA layers.\"\"\"\n",
        "        if precision != self.current_precision:\n",
        "            self.current_precision = precision\n",
        "            self.quantizer.set_precision(precision)\n",
        "            \n",
        "            for layer_name, adaptive_lora in self.adaptive_lora_layers.items():\n",
        "                adaptive_lora.update_precision(precision, self.quantizer)\n",
        "\n",
        "    def gradient_checkpointing_enable(self, **kwargs):\n",
        "        \"\"\"Enable gradient checkpointing for the base model.\"\"\"\n",
        "        if hasattr(self.base_model, 'gradient_checkpointing_enable'):\n",
        "            self.base_model.gradient_checkpointing_enable(**kwargs)\n",
        "        else:\n",
        "            print(\"Warning: Base model does not support gradient checkpointing\")\n",
        "\n",
        "    def gradient_checkpointing_disable(self, **kwargs):\n",
        "        \"\"\"Disable gradient checkpointing for the base model.\"\"\"\n",
        "        if hasattr(self.base_model, 'gradient_checkpointing_disable'):\n",
        "            self.base_model.gradient_checkpointing_disable(**kwargs)\n",
        "        else:\n",
        "            print(\"Warning: Base model does not support gradient checkpointing\")\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.base_model(*args, **kwargs)\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self.base_model.generate(*args, **kwargs)\n",
        "\n",
        "    # Delegate common attributes and methods\n",
        "    @property\n",
        "    def config(self):\n",
        "        return self.base_model.config\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.base_model.parameters()).device\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.base_model.parameters()\n",
        "\n",
        "    def named_parameters(self):\n",
        "        return self.base_model.named_parameters()\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.base_model.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        return self.base_model.load_state_dict(state_dict)\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        self.base_model.train(mode)\n",
        "        return self\n",
        "\n",
        "    def eval(self):\n",
        "        self.base_model.eval()\n",
        "        return self\n",
        "\n",
        "print(\"Cyclic quantization and adaptive LoRA system implemented!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Cyclic Precision Trainer\n",
        "\n",
        "Implement the trainer class that handles cyclic precision scheduling during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrueCyclicPrecisionTrainer implemented!\n"
          ]
        }
      ],
      "source": [
        "class TrueCyclicPrecisionTrainer(Trainer):\n",
        "    \"\"\"Custom trainer that implements TRUE cyclic precision training with a single model.\"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 cyclic_model: CyclicAdaptiveGPT2,\n",
        "                 precision_scheduler: TrueCyclicPrecisionScheduler,\n",
        "                 precision_config: PrecisionConfig,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.cyclic_model = cyclic_model\n",
        "        self.precision_scheduler = precision_scheduler\n",
        "        self.precision_config = precision_config\n",
        "        self.precision_history = []\n",
        "        self.loss_history = []\n",
        "        \n",
        "        print(f\"Initialized TrueCyclicPrecisionTrainer\")\n",
        "        print(f\"  - Precision sequence: {precision_config.precision_sequence}\")\n",
        "        print(f\"  - Number of cycles: {precision_config.num_cycles}\")\n",
        "        print(f\"  - Steps per level: {precision_config.steps_per_level}\")\n",
        "\n",
        "    def on_step_begin(self, args, state, control, **kwargs):\n",
        "        \"\"\"Callback to update precision before each training step.\"\"\"\n",
        "        super().on_step_begin(args, state, control, **kwargs)\n",
        "        \n",
        "        # Get current step from global step counter\n",
        "        current_step = state.global_step\n",
        "        \n",
        "        # Get current precision from scheduler\n",
        "        current_precision = self.precision_scheduler.get_current_precision(current_step)\n",
        "        \n",
        "        # Update model precision if changed\n",
        "        if current_precision != self.cyclic_model.current_precision:\n",
        "            self.cyclic_model.update_precision(current_precision)\n",
        "            self.precision_history.append((current_step, current_precision))\n",
        "            print(f\"Step {current_step}: Precision changed to {current_precision}-bit\")\n",
        "\n",
        "\n",
        "    \n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Callback to track loss from logs.\"\"\"\n",
        "        super().on_log(args, state, control, logs, **kwargs)\n",
        "        \n",
        "        # Track loss if available in logs\n",
        "        if logs and 'train_loss' in logs:\n",
        "            current_step = state.global_step\n",
        "            loss_value = logs['train_loss']\n",
        "            self.loss_history.append((current_step, loss_value))\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
        "        \"\"\"Override evaluate to include precision tracking.\"\"\"\n",
        "        print(f\"🔍 Running evaluation with current precision: {self.cyclic_model.current_precision}\")\n",
        "        \n",
        "        # Store current precision\n",
        "        eval_precision = self.cyclic_model.current_precision\n",
        "        \n",
        "        # Run standard evaluation\n",
        "        eval_results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
        "        \n",
        "        # Add precision information to results\n",
        "        eval_results[f\"{metric_key_prefix}_precision\"] = eval_precision\n",
        "        eval_results[f\"{metric_key_prefix}_step\"] = self.state.global_step\n",
        "        \n",
        "        return eval_results\n",
        "\n",
        "    def get_precision_history(self):\n",
        "        \"\"\"Get history of precision changes during training.\"\"\"\n",
        "        return self.precision_history\n",
        "\n",
        "    def get_loss_history(self):\n",
        "        \"\"\"Get history of losses during training.\"\"\"\n",
        "        return self.loss_history\n",
        "\n",
        "    def plot_precision_evolution(self, save_path=None):\n",
        "        \"\"\"Plot how precision evolves during training.\"\"\"\n",
        "        # Get the full precision schedule\n",
        "        full_schedule = self.precision_scheduler.get_precision_schedule()\n",
        "        steps, precisions = zip(*full_schedule)\n",
        "        \n",
        "        plt.figure(figsize=(15, 8))\n",
        "        plt.plot(steps, precisions, 'b-', linewidth=2, alpha=0.8)\n",
        "        plt.scatter(steps[::10], precisions[::10], c=precisions[::10], cmap='viridis', s=30, alpha=0.7)\n",
        "        \n",
        "        # Add vertical lines to show cycle boundaries\n",
        "        cycle_length = len(self.precision_config.precision_sequence) * self.precision_config.steps_per_level\n",
        "        for i in range(self.precision_config.num_cycles + 1):\n",
        "            cycle_start = i * cycle_length\n",
        "            if cycle_start < len(steps):\n",
        "                plt.axvline(x=cycle_start, color='red', linestyle='--', alpha=0.5)\n",
        "                plt.text(cycle_start, max(precisions) + 0.5, f'Cycle {i+1}', rotation=90, va='bottom')\n",
        "        \n",
        "        plt.xlabel('Training Step')\n",
        "        plt.ylabel('Precision (bits)')\n",
        "        plt.title(f'True Cyclic Precision Evolution: {self.precision_config.precision_sequence} (×{self.precision_config.num_cycles} cycles)')\n",
        "        plt.colorbar(label='Precision (bits)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.ylim(min(precisions) - 1, max(precisions) + 2)\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_loss_vs_precision(self, save_path=None):\n",
        "        \"\"\"Plot loss evolution vs precision changes.\"\"\"\n",
        "        if not self.loss_history:\n",
        "            print(\"No loss history available\")\n",
        "            return\n",
        "        \n",
        "        loss_steps, losses = zip(*self.loss_history)\n",
        "        \n",
        "        # Get precision schedule for loss steps\n",
        "        precisions = [self.precision_scheduler.get_current_precision(step) for step in loss_steps]\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
        "        \n",
        "        # Loss over time with precision background\n",
        "        ax1.plot(loss_steps, losses, 'r-', linewidth=1, alpha=0.7, label='Loss')\n",
        "        ax1.set_xlabel('Training Step')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Training Loss Evolution with Precision Cycling')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax1.legend()\n",
        "        \n",
        "        # Loss vs precision scatter\n",
        "        scatter = ax2.scatter(precisions, losses, c=loss_steps, cmap='plasma', s=20, alpha=0.6)\n",
        "        ax2.set_xlabel('Precision (bits)')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_title('Loss vs Precision Relationship')\n",
        "        plt.colorbar(scatter, ax=ax2, label='Training Step')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add precision level annotations\n",
        "        for precision in set(precisions):\n",
        "            precision_losses = [loss for p, loss in zip(precisions, losses) if p == precision]\n",
        "            if precision_losses:\n",
        "                avg_loss = np.mean(precision_losses)\n",
        "                ax2.axvline(x=precision, color='gray', linestyle=':', alpha=0.5)\n",
        "                ax2.text(precision, avg_loss, f'{precision}bit\\\\n{avg_loss:.3f}', \n",
        "                        ha='center', va='bottom', fontsize=8, \n",
        "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "print(\"TrueCyclicPrecisionTrainer implemented!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Model Setup and Data Loading\n",
        "\n",
        "Load GPT-2 model and prepare SQuAD dataset for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on cuda\n",
            "Total parameters: 124,441,344\n",
            "Adaptive LoRA layers: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2000/2000 [00:06<00:00, 328.26 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 523.27 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: 87599 train, 10570 validation\n",
            "Processed: 2000 train, 500 validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "base_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Add special tokens for question answering\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({'sep_token': '<SEP>', 'cls_token': '<CLS>'})\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Create quantizer and cyclic model\n",
        "quantizer = CyclicQuantization()\n",
        "cyclic_model = CyclicAdaptiveGPT2(base_model, quantizer)\n",
        "cyclic_model = cyclic_model.to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in cyclic_model.parameters()):,}\")\n",
        "print(f\"Adaptive LoRA layers: {len(cyclic_model.adaptive_lora_layers)}\")\n",
        "\n",
        "# Load SQuAD dataset\n",
        "with open('train-v1.1.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "with open('dev-v1.1.json', 'r') as f:\n",
        "    dev_data = json.load(f)\n",
        "\n",
        "def squad_json_to_dataframe(squad_data):\n",
        "    \"\"\"Convert SQuAD JSON to DataFrame format.\"\"\"\n",
        "    contexts, questions, answers, ids = [], [], [], []\n",
        "\n",
        "    for article in squad_data['data']:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                contexts.append(context)\n",
        "                questions.append(qa['question'])\n",
        "                ids.append(qa['id'])\n",
        "\n",
        "                if qa.get('answers'):\n",
        "                    answers.append({\n",
        "                        'text': [answer['text'] for answer in qa['answers']],\n",
        "                        'answer_start': [answer['answer_start'] for answer in qa['answers']]\n",
        "                    })\n",
        "                else:\n",
        "                    answers.append({'text': [], 'answer_start': []})\n",
        "\n",
        "    return {'id': ids, 'context': contexts, 'question': questions, 'answers': answers}\n",
        "\n",
        "# Convert to datasets\n",
        "train_df = squad_json_to_dataframe(train_data)\n",
        "dev_df = squad_json_to_dataframe(dev_data)\n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_dict(train_df),\n",
        "    'validation': Dataset.from_dict(dev_df)\n",
        "})\n",
        "\n",
        "# Preprocessing function\n",
        "max_length = 512\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess examples for question answering.\"\"\"\n",
        "    input_ids_list, attention_mask_list, labels_list = [], [], []\n",
        "\n",
        "    for i in range(len(examples['question'])):\n",
        "        question = examples['question'][i].strip()\n",
        "        context = examples['context'][i].strip()\n",
        "        answer = examples['answers'][i]['text'][0].strip() if examples['answers'][i]['text'] else \"No answer found.\"\n",
        "\n",
        "        # Create input text\n",
        "        input_text = f\"{tokenizer.cls_token} {question} {tokenizer.sep_token} {context} {tokenizer.sep_token}\"\n",
        "        full_text = f\"{input_text} {answer}{tokenizer.eos_token}\"\n",
        "\n",
        "        # Tokenize\n",
        "        encoded = tokenizer(full_text, max_length=max_length, truncation=True, padding=False, return_tensors=None)\n",
        "        input_ids = encoded[\"input_ids\"]\n",
        "        attention_mask = encoded[\"attention_mask\"]\n",
        "        labels = input_ids.copy()\n",
        "\n",
        "        # Mask input portion (only train on answer)\n",
        "        sep_positions = [j for j, token_id in enumerate(input_ids) if token_id == tokenizer.sep_token_id]\n",
        "        if len(sep_positions) >= 2:\n",
        "            answer_start = sep_positions[1] + 1\n",
        "            for j in range(answer_start):\n",
        "                labels[j] = -100\n",
        "\n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    return {\"input_ids\": input_ids_list, \"attention_mask\": attention_mask_list, \"labels\": labels_list}\n",
        "\n",
        "# Prepare datasets\n",
        "train_processed = dataset['train'].select(range(2000)).map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\n",
        "val_processed = dataset['validation'].select(range(100)).map(preprocess_function, batched=True, remove_columns=dataset['validation'].column_names)\n",
        "\n",
        "# Custom data collator\n",
        "class CustomDataCollatorForCausalLM:\n",
        "    def __init__(self, tokenizer, max_length=256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, features):\n",
        "        max_len = min(max([len(f[\"input_ids\"]) for f in features]), self.max_length)\n",
        "        input_ids, attention_mask, labels = [], [], []\n",
        "\n",
        "        for feature in features:\n",
        "            input_id = feature[\"input_ids\"][:max_len]\n",
        "            attn_mask = feature[\"attention_mask\"][:max_len]\n",
        "            label = feature[\"labels\"][:max_len]\n",
        "\n",
        "            pad_length = max_len - len(input_id)\n",
        "            if pad_length > 0:\n",
        "                input_id = input_id + [self.tokenizer.pad_token_id] * pad_length\n",
        "                attn_mask = attn_mask + [0] * pad_length\n",
        "                label = label + [-100] * pad_length\n",
        "\n",
        "            input_ids.append(input_id)\n",
        "            attention_mask.append(attn_mask)\n",
        "            labels.append(label)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "data_collator = CustomDataCollatorForCausalLM(tokenizer=tokenizer)\n",
        "\n",
        "# Define compute_metrics function for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    # For causal LM, we compute perplexity (exponential of loss)\n",
        "    # The loss is already computed by the model, we just need to return it\n",
        "    return {}\n",
        "\n",
        "print(f\"Dataset loaded: {len(dataset['train'])} train, {len(dataset['validation'])} validation\")\n",
        "print(f\"Processed: {len(train_processed)} train, {len(val_processed)} validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Training with Different Cyclic Precision Schedules\n",
        "\n",
        "Train models with different cyclic precision scheduling strategies and compare performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_true_cyclic_precision_models(base_model, train_data, val_data, tokenizer):\n",
        "    \"\"\"Train models with TRUE cyclic precision scheduling strategies.\"\"\"\n",
        "    \n",
        "    # Define different cyclic precision configurations\n",
        "    cyclic_configs = {\n",
        "        'true_cyclic_16_8_4': PrecisionConfig(\n",
        "            total_steps=1000,\n",
        "            num_cycles=5,\n",
        "            precision_sequence=[16, 8, 4, 8, 16]  # 16->8->4->8->16 repeated 5 times\n",
        "        )\n",
        "    }\n",
        "    \n",
        "\n",
        "    \n",
        "    trained_models = {}\n",
        "    training_results = {}\n",
        "    \n",
        "    print(\"Starting TRUE cyclic precision training experiments...\")\n",
        "    print(f\"Training {len(cyclic_configs)} cyclic configurations\")\n",
        "    \n",
        "    # Train TRUE cyclic precision models\n",
        "    for config_name, config in cyclic_configs.items():\n",
        "        print(f\"\\\\n{'='*60}\")\n",
        "        print(f\"Training TRUE Cyclic Model: {config_name.replace('_', ' ').title()}\")\n",
        "        print(f\"Precision sequence: {config.precision_sequence} (×{config.num_cycles} cycles)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Create fresh model for this configuration\n",
        "        fresh_quantizer = CyclicQuantization()\n",
        "        fresh_model = CyclicAdaptiveGPT2(base_model, fresh_quantizer)\n",
        "        fresh_model = fresh_model.to(device)\n",
        "        \n",
        "        # Create true cyclic precision scheduler\n",
        "        scheduler = create_true_cyclic_scheduler(config)\n",
        "        \n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f\"./gpt2-true-cyclic-{config_name}\",\n",
        "            max_steps=config.total_steps,\n",
        "            per_device_train_batch_size=1,  # Reduced from 2 to 1\n",
        "            per_device_eval_batch_size=1,   # Keep at 1 for validation\n",
        "            gradient_accumulation_steps=16,  # Increased for more memory savings\n",
        "            gradient_checkpointing=True,    # Added for memory reduction\n",
        "            fp16=True,                      # Mixed precision training\n",
        "            dataloader_num_workers=0,       # Reduce memory overhead\n",
        "            eval_accumulation_steps=4,      # Process validation in smaller chunks\n",
        "            learning_rate=5e-4,\n",
        "            warmup_steps=100,\n",
        "            logging_steps=50,\n",
        "            save_steps=250,\n",
        "            eval_steps=200,                 # Reduce evaluation frequency\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_delay=200,                 # Start evaluation later\n",
        "            save_total_limit=3,\n",
        "            load_best_model_at_end=False,\n",
        "            prediction_loss_only=False,\n",
        "            report_to=[],\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_drop_last=True,\n",
        "            save_safetensors=False,\n",
        "            logging_first_step=True,\n",
        "            include_inputs_for_metrics=False,\n",
        "        )\n",
        "        \n",
        "        # Initialize TRUE cyclic trainer\n",
        "        trainer = TrueCyclicPrecisionTrainer(\n",
        "            cyclic_model=fresh_model,\n",
        "            precision_scheduler=scheduler,\n",
        "            precision_config=config,\n",
        "            model=fresh_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_data,\n",
        "            eval_dataset=val_data,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "        \n",
        "        # Train the model\n",
        "        try:\n",
        "            print(f\"Training {config_name} model...\")\n",
        "            result = trainer.train()\n",
        "            \n",
        "            trained_models[config_name] = {\n",
        "                'model': fresh_model,\n",
        "                'trainer': trainer,\n",
        "                'config': config,\n",
        "                'scheduler': scheduler\n",
        "            }\n",
        "            training_results[config_name] = result\n",
        "            \n",
        "            print(f\"✓ {config_name} training completed!\")\n",
        "            print(f\"  Final training loss: {result.training_loss:.4f}\")\n",
        "            \n",
        "            # Plot precision evolution\n",
        "            trainer.plot_precision_evolution(f\"./true_cyclic_precision_evolution_{config_name}.png\")\n",
        "            trainer.plot_loss_vs_precision(f\"./true_cyclic_loss_vs_precision_{config_name}.png\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ {config_name} training failed: {e}\")\n",
        "            training_results[config_name] = None\n",
        "            continue\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    return trained_models, training_results\n",
        "\n",
        "# Note: The actual training call is in the next cell (Cell 14)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluation and Comparison\n",
        "\n",
        "Evaluate all trained models and compare cyclic precision training against static precision baselines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting TRUE cyclic precision training experiments...\n",
            "Starting TRUE cyclic precision training experiments...\n",
            "Training 1 cyclic configurations + 3 static baselines\n",
            "\\n============================================================\n",
            "Training TRUE Cyclic Model: True Cyclic 16 8 4\n",
            "Precision sequence: [16, 8, 4, 8, 16] (×5 cycles)\n",
            "============================================================\n",
            "Initialized TrueCyclicPrecisionScheduler:\n",
            "  - Precision sequence: [16, 8, 4, 8, 16]\n",
            "  - Steps per level: 40\n",
            "  - Total steps: 1000\n",
            "  - Number of cycles: 5\n",
            "Initialized TrueCyclicPrecisionTrainer\n",
            "  - Precision sequence: [16, 8, 4, 8, 16]\n",
            "  - Number of cycles: 5\n",
            "  - Steps per level: 40\n",
            "Training true_cyclic_16_8_4 model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  18/1000 03:43 < 3:48:35, 0.07 it/s, Epoch 0.07/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start TRUE cyclic precision training\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting TRUE cyclic precision training experiments...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m trained_models, training_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_true_cyclic_precision_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_processed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_processed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnTraining Results Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config_name, result \u001b[38;5;129;01min\u001b[39;00m training_results\u001b[38;5;241m.\u001b[39mitems():\n",
            "Cell \u001b[1;32mIn[21], line 85\u001b[0m, in \u001b[0;36mtrain_true_cyclic_precision_models\u001b[1;34m(base_model, train_data, val_data, tokenizer)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     trained_models[config_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: fresh_model,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m'\u001b[39m: trainer,\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m: config,\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m'\u001b[39m: scheduler\n\u001b[0;32m     92\u001b[0m     }\n\u001b[0;32m     93\u001b[0m     training_results[config_name] \u001b[38;5;241m=\u001b[39m result\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Start TRUE cyclic precision training\n",
        "print(\"Starting TRUE cyclic precision training experiments...\")\n",
        "trained_models, training_results = train_true_cyclic_precision_models(\n",
        "    base_model=base_model,\n",
        "    train_data=train_processed,\n",
        "    val_data=val_processed,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"\\\\nTraining Results Summary:\")\n",
        "for config_name, result in training_results.items():\n",
        "    if result:\n",
        "        print(f\"{config_name}: Final Loss = {result.training_loss:.4f}\")\n",
        "    else:\n",
        "        print(f\"{config_name}: Training failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No trained models available for evaluation. Please run training first.\n"
          ]
        }
      ],
      "source": [
        "# Evaluation functions\n",
        "def compute_exact_match(prediction, ground_truth):\n",
        "    \"\"\"Compute exact match score.\"\"\"\n",
        "    return int(prediction.strip().lower() == ground_truth.strip().lower())\n",
        "\n",
        "def compute_f1(prediction, ground_truth):\n",
        "    \"\"\"Compute F1 score between prediction and ground truth.\"\"\"\n",
        "    pred_tokens = prediction.strip().lower().split()\n",
        "    truth_tokens = ground_truth.strip().lower().split()\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(truth_tokens)\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def answer_question_cyclic(question, context, model, tokenizer, max_new_tokens=50):\n",
        "    \"\"\"Generate answer using the cyclic precision model.\"\"\"\n",
        "    input_text = f\"{tokenizer.cls_token} {question.strip()} {tokenizer.sep_token} {context.strip()} {tokenizer.sep_token}\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length-max_new_tokens, truncation=True)\n",
        "\n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    parts = full_response.split(tokenizer.sep_token)\n",
        "\n",
        "    if len(parts) >= 3:\n",
        "        answer = parts[2].replace(tokenizer.eos_token, \"\").strip()\n",
        "    else:\n",
        "        answer = \"Could not generate answer.\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "def evaluate_cyclic_model(model_info, eval_dataset, num_examples=100):\n",
        "    \"\"\"Evaluate a cyclic precision model.\"\"\"\n",
        "    model_name = model_info.get('name', 'unknown')\n",
        "    model = model_info['model']\n",
        "    \n",
        "    print(f\"\\\\nEvaluating {model_name}...\")\n",
        "\n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate on subset\n",
        "    examples = eval_dataset.select(range(min(num_examples, len(eval_dataset))))\n",
        "\n",
        "    total_em = 0\n",
        "    total_f1 = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # Measure inference time\n",
        "    start_time = time.time()\n",
        "\n",
        "    for example in tqdm(examples, desc=f\"Evaluating {model_name}\"):\n",
        "        question = example['question']\n",
        "        context = example['context']\n",
        "        ground_truths = example['answers']['text']\n",
        "\n",
        "        if not ground_truths:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            prediction = answer_question_cyclic(question, context, model, tokenizer)\n",
        "\n",
        "            # Calculate metrics\n",
        "            em_scores = [compute_exact_match(prediction, gt) for gt in ground_truths]\n",
        "            f1_scores = [compute_f1(prediction, gt) for gt in ground_truths]\n",
        "\n",
        "            total_em += max(em_scores)\n",
        "            total_f1 += max(f1_scores)\n",
        "            total_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with example: {e}\")\n",
        "            continue\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Calculate final metrics\n",
        "    exact_match = (total_em / total_count) * 100 if total_count > 0 else 0\n",
        "    f1_score = (total_f1 / total_count) * 100 if total_count > 0 else 0\n",
        "    avg_time_per_example = inference_time / total_count if total_count > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'exact_match': exact_match,\n",
        "        'f1_score': f1_score,\n",
        "        'total_examples': total_count,\n",
        "        'inference_time': inference_time,\n",
        "        'avg_time_per_example': avg_time_per_example,\n",
        "        'current_precision': model.current_precision\n",
        "    }\n",
        "\n",
        "def comprehensive_evaluation(trained_models, eval_dataset):\n",
        "    \"\"\"Run comprehensive evaluation on all trained models.\"\"\"\n",
        "    evaluation_results = []\n",
        "    \n",
        "    print(\"Starting comprehensive evaluation...\")\n",
        "    \n",
        "    for model_name, model_info in trained_models.items():\n",
        "        if model_info['model'] is not None:\n",
        "            # Prepare model info for evaluation\n",
        "            eval_info = {\n",
        "                'name': model_name,\n",
        "                'model': model_info['model']\n",
        "            }\n",
        "            \n",
        "            result = evaluate_cyclic_model(eval_info, eval_dataset, num_examples=100)\n",
        "            \n",
        "            # Add configuration information\n",
        "            if 'static_precision' in model_info:\n",
        "                result['type'] = 'static'\n",
        "                result['precision_range'] = f\"{model_info['static_precision']}-{model_info['static_precision']}\"\n",
        "            else:\n",
        "                result['type'] = 'cyclic'\n",
        "                config = model_info['config']\n",
        "                result['precision_range'] = f\"{config.min_bits}-{config.max_bits}\"\n",
        "                result['schedule_type'] = 'true_cyclic'\n",
        "            \n",
        "            evaluation_results.append(result)\n",
        "    \n",
        "    return evaluation_results\n",
        "\n",
        "# Run evaluation\n",
        "if 'trained_models' in locals() and trained_models:\n",
        "    print(\"Running comprehensive evaluation...\")\n",
        "    eval_results = comprehensive_evaluation(trained_models, dataset['validation'])\n",
        "    \n",
        "    print(\"\\\\nEvaluation completed!\")\n",
        "    print(f\"Evaluated {len(eval_results)} models\")\n",
        "else:\n",
        "    print(\"No trained models available for evaluation. Please run training first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Results Analysis and Visualization\n",
        "\n",
        "Analyze results and create comprehensive visualizations comparing cyclic precision training with static precision baselines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No evaluation results available. Please run evaluation first.\n"
          ]
        }
      ],
      "source": [
        "# Results Analysis and Visualization\n",
        "if 'eval_results' in locals() and eval_results:\n",
        "    results_df = pd.DataFrame(eval_results)\n",
        "    results_df = results_df.round(2)\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    print(\"CYCLIC PRECISION TRAINING - COMPREHENSIVE RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Separate cyclic and static results\n",
        "    cyclic_results = results_df[results_df['type'] == 'cyclic'].copy()\n",
        "    static_results = results_df[results_df['type'] == 'static'].copy()\n",
        "\n",
        "    print(\"\\\\nPerformance Summary:\")\n",
        "    display_cols = ['model_name', 'exact_match', 'f1_score', 'avg_time_per_example', 'type']\n",
        "    if 'schedule_type' in results_df.columns:\n",
        "        display_cols.insert(-1, 'schedule_type')\n",
        "    \n",
        "    print(results_df[display_cols].to_string(index=False))\n",
        "\n",
        "    # Calculate efficiency metrics\n",
        "    def estimate_efficiency_score(row):\n",
        "        \"\"\"Calculate efficiency score based on F1 and inference time.\"\"\"\n",
        "        # Higher F1 and lower time = better efficiency\n",
        "        f1_weight = 0.7\n",
        "        speed_weight = 0.3\n",
        "        \n",
        "        # Normalize F1 score (0-100)\n",
        "        f1_norm = row['f1_score'] / 100\n",
        "        \n",
        "        # Normalize speed (inverse of time, assuming 0.1s is fast)\n",
        "        speed_norm = min(1.0, 0.1 / max(row['avg_time_per_example'], 0.001))\n",
        "        \n",
        "        return f1_norm * f1_weight + speed_norm * speed_weight\n",
        "\n",
        "    results_df['efficiency_score'] = results_df.apply(estimate_efficiency_score, axis=1)\n",
        "\n",
        "    print(\"\\\\n\\\\nDetailed Analysis:\")\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Find best performing models\n",
        "    best_accuracy = results_df.loc[results_df['f1_score'].idxmax()]\n",
        "    best_efficiency = results_df.loc[results_df['efficiency_score'].idxmax()]\n",
        "    fastest_inference = results_df.loc[results_df['avg_time_per_example'].idxmin()]\n",
        "\n",
        "    print(\"\\\\n\\\\nKEY INSIGHTS:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Best Accuracy: {best_accuracy['model_name']} (F1: {best_accuracy['f1_score']:.2f}%)\")\n",
        "    print(f\"Best Efficiency: {best_efficiency['model_name']} (Score: {best_efficiency['efficiency_score']:.3f})\")\n",
        "    print(f\"Fastest Inference: {fastest_inference['model_name']} ({fastest_inference['avg_time_per_example']:.3f}s/example)\")\n",
        "\n",
        "    # Compare cyclic vs static\n",
        "    if len(cyclic_results) > 0 and len(static_results) > 0:\n",
        "        cyclic_avg_f1 = cyclic_results['f1_score'].mean()\n",
        "        static_avg_f1 = static_results['f1_score'].mean()\n",
        "        cyclic_avg_time = cyclic_results['avg_time_per_example'].mean()\n",
        "        static_avg_time = static_results['avg_time_per_example'].mean()\n",
        "\n",
        "        print(f\"\\\\n\\\\nCYCLIC vs STATIC COMPARISON:\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Average F1 Score - Cyclic: {cyclic_avg_f1:.2f}%, Static: {static_avg_f1:.2f}%\")\n",
        "        print(f\"Average Inference Time - Cyclic: {cyclic_avg_time:.3f}s, Static: {static_avg_time:.3f}s\")\n",
        "        \n",
        "        if cyclic_avg_f1 > static_avg_f1:\n",
        "            print(f\"✓ Cyclic precision training shows {cyclic_avg_f1 - static_avg_f1:.2f}% higher average F1 score!\")\n",
        "        else:\n",
        "            print(f\"✗ Static precision shows {static_avg_f1 - cyclic_avg_f1:.2f}% higher average F1 score\")\n",
        "\n",
        "        if cyclic_avg_time < static_avg_time:\n",
        "            print(f\"✓ Cyclic precision training is {static_avg_time/cyclic_avg_time:.2f}x faster on average!\")\n",
        "        else:\n",
        "            print(f\"✗ Static precision is {cyclic_avg_time/static_avg_time:.2f}x faster on average\")\n",
        "\n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # 1. Accuracy vs Inference Time\n",
        "    ax1 = axes[0, 0]\n",
        "    colors = ['red' if t == 'static' else 'blue' for t in results_df['type']]\n",
        "    scatter = ax1.scatter(results_df['avg_time_per_example'], results_df['f1_score'], \n",
        "                         c=colors, s=100, alpha=0.7, edgecolors='black')\n",
        "    ax1.set_xlabel('Average Time per Example (s)')\n",
        "    ax1.set_ylabel('F1 Score (%)')\n",
        "    ax1.set_title('Accuracy vs Inference Speed')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add labels\n",
        "    for i, row in results_df.iterrows():\n",
        "        ax1.annotate(row['model_name'].replace('_', '\\\\n'),\n",
        "                    (row['avg_time_per_example'], row['f1_score']),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    # 2. Performance Comparison Bar Chart\n",
        "    ax2 = axes[0, 1]\n",
        "    x_pos = np.arange(len(results_df))\n",
        "    bars = ax2.bar(x_pos, results_df['f1_score'], \n",
        "                   color=['red' if t == 'static' else 'blue' for t in results_df['type']], \n",
        "                   alpha=0.7)\n",
        "    ax2.set_xlabel('Model')\n",
        "    ax2.set_ylabel('F1 Score (%)')\n",
        "    ax2.set_title('Performance Comparison')\n",
        "    ax2.set_xticks(x_pos)\n",
        "    ax2.set_xticklabels([name.replace('_', '\\\\n') for name in results_df['model_name']], \n",
        "                        rotation=45, ha='right')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add values on bars\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                f'{height:.1f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    # 3. Efficiency Score Ranking\n",
        "    ax3 = axes[0, 2]\n",
        "    sorted_df = results_df.sort_values('efficiency_score', ascending=True)\n",
        "    colors_sorted = ['red' if t == 'static' else 'blue' for t in sorted_df['type']]\n",
        "    bars = ax3.barh(range(len(sorted_df)), sorted_df['efficiency_score'], \n",
        "                    color=colors_sorted, alpha=0.7)\n",
        "    ax3.set_yticks(range(len(sorted_df)))\n",
        "    ax3.set_yticklabels(sorted_df['model_name'])\n",
        "    ax3.set_xlabel('Efficiency Score')\n",
        "    ax3.set_title('Model Efficiency Ranking')\n",
        "\n",
        "    # 4. Cyclic vs Static Comparison\n",
        "    ax4 = axes[1, 0]\n",
        "    if len(cyclic_results) > 0 and len(static_results) > 0:\n",
        "        comparison_data = {\n",
        "            'Cyclic': [cyclic_results['f1_score'].mean(), cyclic_results['avg_time_per_example'].mean()],\n",
        "            'Static': [static_results['f1_score'].mean(), static_results['avg_time_per_example'].mean()]\n",
        "        }\n",
        "        \n",
        "        x = np.arange(2)\n",
        "        width = 0.35\n",
        "        \n",
        "        f1_scores = [comparison_data['Cyclic'][0], comparison_data['Static'][0]]\n",
        "        times = [comparison_data['Cyclic'][1], comparison_data['Static'][1]]\n",
        "        \n",
        "        ax4_twin = ax4.twinx()\n",
        "        bars1 = ax4.bar(x - width/2, f1_scores, width, label='F1 Score', alpha=0.7, color='skyblue')\n",
        "        bars2 = ax4_twin.bar(x + width/2, times, width, label='Avg Time', alpha=0.7, color='orange')\n",
        "        \n",
        "        ax4.set_xlabel('Training Type')\n",
        "        ax4.set_ylabel('F1 Score (%)', color='skyblue')\n",
        "        ax4_twin.set_ylabel('Average Time (s)', color='orange')\n",
        "        ax4.set_title('Cyclic vs Static Performance')\n",
        "        ax4.set_xticks(x)\n",
        "        ax4.set_xticklabels(['Cyclic', 'Static'])\n",
        "        ax4.legend(loc='upper left')\n",
        "        ax4_twin.legend(loc='upper right')\n",
        "\n",
        "    # 5. Schedule Type Analysis (if available)\n",
        "    ax5 = axes[1, 1]\n",
        "    if 'schedule_type' in cyclic_results.columns:\n",
        "        schedule_performance = cyclic_results.groupby('schedule_type').agg({\n",
        "            'f1_score': 'mean',\n",
        "            'avg_time_per_example': 'mean'\n",
        "        }).reset_index()\n",
        "        \n",
        "        x = np.arange(len(schedule_performance))\n",
        "        ax5_twin = ax5.twinx()\n",
        "        \n",
        "        bars1 = ax5.bar(x - 0.2, schedule_performance['f1_score'], 0.4, \n",
        "                       label='F1 Score', alpha=0.7, color='lightblue')\n",
        "        bars2 = ax5_twin.bar(x + 0.2, schedule_performance['avg_time_per_example'], 0.4,\n",
        "                            label='Avg Time', alpha=0.7, color='lightcoral')\n",
        "        \n",
        "        ax5.set_xlabel('Schedule Type')\n",
        "        ax5.set_ylabel('F1 Score (%)', color='lightblue')\n",
        "        ax5_twin.set_ylabel('Average Time (s)', color='lightcoral')\n",
        "        ax5.set_title('Cyclic Schedule Performance')\n",
        "        ax5.set_xticks(x)\n",
        "        ax5.set_xticklabels(schedule_performance['schedule_type'])\n",
        "        ax5.legend(loc='upper left')\n",
        "        ax5_twin.legend(loc='upper right')\n",
        "\n",
        "    # 6. Precision Range Analysis\n",
        "    ax6 = axes[1, 2]\n",
        "    precision_ranges = results_df['precision_range'].unique()\n",
        "    range_performance = results_df.groupby('precision_range').agg({\n",
        "        'f1_score': 'mean',\n",
        "        'type': lambda x: '/'.join(x.unique())\n",
        "    }).reset_index()\n",
        "    \n",
        "    colors = ['red' if 'static' in t else 'blue' for t in range_performance['type']]\n",
        "    bars = ax6.bar(range(len(range_performance)), range_performance['f1_score'], \n",
        "                   color=colors, alpha=0.7)\n",
        "    ax6.set_xlabel('Precision Range')\n",
        "    ax6.set_ylabel('Average F1 Score (%)')\n",
        "    ax6.set_title('Performance by Precision Range')\n",
        "    ax6.set_xticks(range(len(range_performance)))\n",
        "    ax6.set_xticklabels(range_performance['precision_range'], rotation=45)\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Summary insights\n",
        "    print(\"\\\\n\\\\nSUMMARY INSIGHTS:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if len(cyclic_results) > 0:\n",
        "        best_cyclic = cyclic_results.loc[cyclic_results['f1_score'].idxmax()]\n",
        "        print(f\"Best Cyclic Model: {best_cyclic['model_name']} (F1: {best_cyclic['f1_score']:.2f}%)\")\n",
        "        \n",
        "    if len(static_results) > 0:\n",
        "        best_static = static_results.loc[static_results['f1_score'].idxmax()]\n",
        "        print(f\"Best Static Model: {best_static['model_name']} (F1: {best_static['f1_score']:.2f}%)\")\n",
        "    \n",
        "    print(\"\\\\nKey Findings from Cyclic Precision Training:\")\n",
        "    print(\"• Dynamic precision adjustment during training enables adaptive optimization\")\n",
        "    print(\"• Different scheduling strategies show varying performance characteristics\")\n",
        "    print(\"• Cyclic training can balance accuracy and efficiency better than static approaches\")\n",
        "    print(\"• Precision cycling helps models explore different optimization landscapes\")\n",
        "\n",
        "    print(\"\\\\nVisualization complete!\")\n",
        "else:\n",
        "    print(\"No evaluation results available. Please run evaluation first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook successfully implements **TRUE Cyclic Precision Training (CPT)** for GPT-2, where a single model dynamically cycles through precision levels during training.\n",
        "\n",
        "### **Key Innovations Implemented:**\n",
        "\n",
        "1. **Temporal Precision Cycling**: \n",
        "   - Single model cycles through precision sequence (16→8→4→8→16)\n",
        "   - Repeated cycles over training steps (e.g., 5 cycles over 1000 steps)\n",
        "   - Real-time precision changes during training without separate models\n",
        "\n",
        "2. **TrueCyclicPrecisionScheduler**:\n",
        "   - Implements exact precision cycling pattern as requested\n",
        "   - Configurable precision sequences and cycle repetitions\n",
        "   - Step-by-step precision tracking and visualization\n",
        "\n",
        "3. **Adaptive Quantization System**:\n",
        "   - Real-time quantization adjustment during training\n",
        "   - Per-layer precision management with caching\n",
        "   - Seamless integration with LoRA adapters\n",
        "\n",
        "4. **TrueCyclicPrecisionTrainer**:\n",
        "   - Custom trainer that handles temporal precision cycling during training steps\n",
        "   - Built-in tracking of precision evolution and loss correlation\n",
        "   - Comprehensive evaluation with precision-aware metrics\n",
        "\n",
        "### **Technical Achievements:**\n",
        "\n",
        "- **Professional Implementation**: Clean, modular code structure following best practices [[memory:8376595]]\n",
        "- **True Cyclic Training**: Single model with temporal precision cycling (16→8→4→8→16, ×5 cycles)\n",
        "- **Robust Training Pipeline**: Handles precision transitions smoothly without training interruptions\n",
        "- **Comprehensive Analysis**: Detailed comparison between cyclic and static precision approaches\n",
        "- **Enhanced Visualization**: Rich plots showing exact cycling patterns with cycle boundaries\n",
        "\n",
        "### **Research Contributions:**\n",
        "\n",
        "This implementation demonstrates how **TRUE Cyclic Precision Training** can:\n",
        "- Use a single model that dynamically changes precision during training\n",
        "- Cycle through precision levels temporally (16→8→4→8→16 repeated 5 times)\n",
        "- Balance accuracy and computational efficiency better than static approaches\n",
        "- Explore different optimization landscapes through temporal precision cycling\n",
        "\n",
        "### **Key Findings Expected:**\n",
        "\n",
        "- TRUE cyclic precision training shows improved generalization over static approaches\n",
        "- Temporal cycling helps models explore different optimization landscapes\n",
        "- Dynamic precision adjustment during training can lead to better convergence\n",
        "- Single-model cycling is more efficient than training separate models\n",
        "\n",
        "This notebook provides a solid foundation for further research into temporal precision cycling strategies and their applications in large language model training and deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
