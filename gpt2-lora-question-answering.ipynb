{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate peft torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    GPT2Config,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset, DatasetDict\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize GPT-2 Model with LoRA Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_checkpoint = \"gpt2\"\n",
        "\n",
        "# Load tokenizer and add special tokens for QA\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({\n",
        "    'sep_token': '<SEP>',\n",
        "    'cls_token': '<CLS>'\n",
        "})\n",
        "\n",
        "# Load base model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_checkpoint)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,  # rank\n",
        "    lora_alpha=32,  # scaling parameter\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # All linear layers in GPT-2\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Explore SQuAD Dataset from Kaggle Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON files\n",
        "with open('train-v1.1.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('dev-v1.1.json', 'r') as f:\n",
        "    dev_data = json.load(f)\n",
        "\n",
        "# Function to convert SQuAD format to flat structure for GPT-2 QA format\n",
        "def squad_json_to_dataframe(squad_data):\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    ids = []\n",
        "    \n",
        "    for article in squad_data['data']:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                contexts.append(context)\n",
        "                questions.append(qa['question'])\n",
        "                ids.append(qa['id'])\n",
        "                \n",
        "                # Handle answers\n",
        "                if qa.get('answers'):\n",
        "                    answer_starts = [answer['answer_start'] for answer in qa['answers']]\n",
        "                    answer_texts = [answer['text'] for answer in qa['answers']]\n",
        "                    answers.append({\n",
        "                        'text': answer_texts,\n",
        "                        'answer_start': answer_starts\n",
        "                    })\n",
        "                else:\n",
        "                    answers.append({\n",
        "                        'text': [],\n",
        "                        'answer_start': []\n",
        "                    })\n",
        "    \n",
        "    return {\n",
        "        'id': ids,\n",
        "        'context': contexts,\n",
        "        'question': questions,\n",
        "        'answers': answers\n",
        "    }\n",
        "\n",
        "# Convert to dataframes\n",
        "train_df = squad_json_to_dataframe(train_data)\n",
        "dev_df = squad_json_to_dataframe(dev_data)\n",
        "\n",
        "# Create datasets\n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_dict(train_df),\n",
        "    'validation': Dataset.from_dict(dev_df)\n",
        "})\n",
        "\n",
        "# Display dataset info\n",
        "print(\"Dataset Structure:\")\n",
        "print(dataset)\n",
        "print(\"\\nTraining examples:\", len(dataset['train']))\n",
        "print(\"Validation examples:\", len(dataset['validation']))\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample training example:\")\n",
        "sample = dataset['train'][0]\n",
        "print(f\"Context: {sample['context'][:200]}...\")\n",
        "print(f\"Question: {sample['question']}\")\n",
        "print(f\"Answer: {sample['answers']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing for GPT-2 QA Format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 512\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Preprocess examples for GPT-2 question answering.\n",
        "    Format: <CLS> Question <SEP> Context <SEP> Answer\n",
        "    \"\"\"\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    labels_list = []\n",
        "    \n",
        "    for i in range(len(examples['question'])):\n",
        "        question = examples['question'][i].strip()\n",
        "        context = examples['context'][i].strip()\n",
        "        \n",
        "        # Get the first answer (if available)\n",
        "        if examples['answers'][i]['text']:\n",
        "            answer = examples['answers'][i]['text'][0].strip()\n",
        "        else:\n",
        "            answer = \"No answer found.\"\n",
        "        \n",
        "        # Create input text in QA format\n",
        "        input_text = f\"{tokenizer.cls_token} {question} {tokenizer.sep_token} {context} {tokenizer.sep_token}\"\n",
        "        full_text = f\"{input_text} {answer}{tokenizer.eos_token}\"\n",
        "        \n",
        "        # Tokenize the full text\n",
        "        encoded = tokenizer(\n",
        "            full_text,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        \n",
        "        input_ids = encoded[\"input_ids\"]\n",
        "        attention_mask = encoded[\"attention_mask\"]\n",
        "        \n",
        "        # Create labels (same as input_ids)\n",
        "        labels = input_ids.copy()\n",
        "        \n",
        "        # Find where the answer starts (after the second SEP token)\n",
        "        sep_positions = [j for j, token_id in enumerate(input_ids) if token_id == tokenizer.sep_token_id]\n",
        "        if len(sep_positions) >= 2:\n",
        "            answer_start = sep_positions[1] + 1\n",
        "            # Mask everything before the answer\n",
        "            for j in range(answer_start):\n",
        "                labels[j] = -100\n",
        "        \n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "        labels_list.append(labels)\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": input_ids_list,\n",
        "        \"attention_mask\": attention_mask_list,\n",
        "        \"labels\": labels_list\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess datasets\n",
        "tokenized_train = dataset[\"train\"].map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    num_proc=2\n",
        ")\n",
        "\n",
        "tokenized_validation = dataset[\"validation\"].map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"validation\"].column_names,\n",
        "    num_proc=2\n",
        ")\n",
        "\n",
        "# Use a smaller subset for faster training (optional)\n",
        "train_size = min(10000, len(tokenized_train))\n",
        "val_size = min(1000, len(tokenized_validation))\n",
        "\n",
        "tokenized_train = tokenized_train.select(range(train_size))\n",
        "tokenized_validation = tokenized_validation.select(range(val_size))\n",
        "\n",
        "print(f\"Training examples: {len(tokenized_train)}\")\n",
        "print(f\"Validation examples: {len(tokenized_validation)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup with LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable wandb logging\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Custom data collator to handle our specific case\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class CustomDataCollatorForCausalLM:\n",
        "    tokenizer: Any\n",
        "    max_length: int = 512\n",
        "    \n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        batch = {}\n",
        "        \n",
        "        # Get the maximum length in this batch\n",
        "        max_len = min(max([len(f[\"input_ids\"]) for f in features]), self.max_length)\n",
        "        \n",
        "        # Pad sequences\n",
        "        input_ids = []\n",
        "        attention_mask = []\n",
        "        labels = []\n",
        "        \n",
        "        for feature in features:\n",
        "            # Pad input_ids\n",
        "            padded_input_ids = feature[\"input_ids\"][:max_len]\n",
        "            padded_input_ids += [self.tokenizer.pad_token_id] * (max_len - len(padded_input_ids))\n",
        "            input_ids.append(padded_input_ids)\n",
        "            \n",
        "            # Pad attention_mask\n",
        "            padded_attention_mask = feature[\"attention_mask\"][:max_len]\n",
        "            padded_attention_mask += [0] * (max_len - len(padded_attention_mask))\n",
        "            attention_mask.append(padded_attention_mask)\n",
        "            \n",
        "            # Pad labels\n",
        "            padded_labels = feature[\"labels\"][:max_len]\n",
        "            padded_labels += [-100] * (max_len - len(padded_labels))\n",
        "            labels.append(padded_labels)\n",
        "        \n",
        "        batch[\"input_ids\"] = torch.tensor(input_ids, dtype=torch.long)\n",
        "        batch[\"attention_mask\"] = torch.tensor(attention_mask, dtype=torch.long)\n",
        "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
        "        \n",
        "        return batch\n",
        "\n",
        "# Training arguments optimized for LoRA fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-lora-qa\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=5e-4,  # Higher LR for LoRA\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=[],  # Explicitly disable all reporting\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Use our custom data collator\n",
        "data_collator = CustomDataCollatorForCausalLM(\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_validation,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model with LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Starting LoRA fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA adapters\n",
        "model.save_pretrained(\"./gpt2-lora-qa-final\")\n",
        "tokenizer.save_pretrained(\"./gpt2-lora-qa-final\")\n",
        "print(\"LoRA adapters saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Training History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract training losses and steps\n",
        "train_data = []\n",
        "eval_data = []\n",
        "\n",
        "for log in log_history:\n",
        "    if 'train_loss' in log and 'step' in log:\n",
        "        train_data.append((log['step'], log['train_loss']))\n",
        "    if 'eval_loss' in log and 'step' in log:\n",
        "        eval_data.append((log['step'], log['eval_loss']))\n",
        "\n",
        "# Separate steps and losses\n",
        "train_steps, train_losses = zip(*train_data) if train_data else ([], [])\n",
        "eval_steps, eval_losses = zip(*eval_data) if eval_data else ([], [])\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Combined plot showing both training and validation losses\n",
        "plt.subplot(1, 3, 1)\n",
        "if train_losses:\n",
        "    plt.plot(train_steps, train_losses, 'bo-', linewidth=2, markersize=8, label='Training Loss')\n",
        "if eval_losses:\n",
        "    plt.plot(eval_steps, eval_losses, 'ro-', linewidth=2, markersize=8, label='Validation Loss')\n",
        "\n",
        "plt.title('Training Progress')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# Training loss only\n",
        "plt.subplot(1, 3, 2)\n",
        "if train_losses:\n",
        "    plt.plot(train_steps, train_losses, 'b-', linewidth=2, marker='o', markersize=8)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    # Add value annotations\n",
        "    for x, y in zip(train_steps, train_losses):\n",
        "        plt.annotate(f'{y:.4f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'No training loss data available', \n",
        "             horizontalalignment='center', verticalalignment='center', \n",
        "             transform=plt.gca().transAxes)\n",
        "    plt.title('Training Loss')\n",
        "\n",
        "# Validation loss only\n",
        "plt.subplot(1, 3, 3)\n",
        "if eval_losses:\n",
        "    plt.plot(eval_steps, eval_losses, 'r-', linewidth=2, marker='o', markersize=8)\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    # Add value annotations\n",
        "    for x, y in zip(eval_steps, eval_losses):\n",
        "        plt.annotate(f'{y:.4f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'No validation loss data available', \n",
        "             horizontalalignment='center', verticalalignment='center', \n",
        "             transform=plt.gca().transAxes)\n",
        "    plt.title('Validation Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print the training metrics table\n",
        "print(\"\\nTraining Metrics Summary:\")\n",
        "print(\"Step\\tTraining Loss\\tValidation Loss\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create a combined view of training and validation losses\n",
        "all_steps = sorted(set(train_steps + eval_steps))\n",
        "for step in all_steps:\n",
        "    train_loss = next((loss for s, loss in train_data if s == step), None)\n",
        "    eval_loss = next((loss for s, loss in eval_data if s == step), None)\n",
        "    \n",
        "    train_str = f\"{train_loss:.6f}\" if train_loss is not None else \"-\"\n",
        "    eval_str = f\"{eval_loss:.6f}\" if eval_loss is not None else \"-\"\n",
        "    \n",
        "    print(f\"{step}\\t{train_str}\\t\\t{eval_str}\")\n",
        "\n",
        "# Analysis of training progress\n",
        "print(f\"\\nTraining Analysis:\")\n",
        "if eval_losses:\n",
        "    print(f\"• Validation loss improved from {eval_losses[0]:.4f} to {eval_losses[-1]:.4f}\")\n",
        "    print(f\"• Total validation loss reduction: {(eval_losses[0] - eval_losses[-1]):.4f}\")\n",
        "    print(f\"• Improvement percentage: {((eval_losses[0] - eval_losses[-1]) / eval_losses[0] * 100):.1f}%\")\n",
        "\n",
        "if train_losses:\n",
        "    print(f\"• Final training loss: {train_losses[-1]:.4f}\")\n",
        "    \n",
        "if train_losses and eval_losses:\n",
        "    # Check if there's overfitting by comparing the last available losses\n",
        "    if train_losses[-1] < eval_losses[-1]:\n",
        "        gap = eval_losses[-1] - train_losses[-1]\n",
        "        print(f\"• Generalization gap: {gap:.4f} (validation loss higher than training loss)\")\n",
        "    else:\n",
        "        print(f\"• Model seems to be generalizing well\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Function for GPT-2 QA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_question_gpt2(question, context, model, tokenizer, max_new_tokens=50):\n",
        "    \"\"\"\n",
        "    Generate answer using fine-tuned GPT-2 with LoRA\n",
        "    \"\"\"\n",
        "    # Format input\n",
        "    input_text = f\"{tokenizer.cls_token} {question.strip()} {tokenizer.sep_token} {context.strip()} {tokenizer.sep_token}\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_length-max_new_tokens,\n",
        "        truncation=True\n",
        "    )\n",
        "    \n",
        "    # Move inputs to the same device as the model\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode and extract answer\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    \n",
        "    # Extract answer part (after the second SEP token)\n",
        "    parts = full_response.split(tokenizer.sep_token)\n",
        "    if len(parts) >= 3:\n",
        "        answer = parts[2].replace(tokenizer.eos_token, \"\").strip()\n",
        "    else:\n",
        "        answer = \"Could not generate answer.\"\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# Test with examples\n",
        "test_examples = [\n",
        "    {\n",
        "        \"context\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889, it was initially criticized by some of France's leading artists and intellectuals.\",\n",
        "        \"question\": \"When was the Eiffel Tower built?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. IBM has a rich history with machine learning.\",\n",
        "        \"question\": \"What is machine learning?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"GPT-2 LoRA Question Answering Examples:\\n\")\n",
        "for example in test_examples:\n",
        "    answer = answer_question_gpt2(example[\"question\"], example[\"context\"], model, tokenizer)\n",
        "    print(f\"Context: {example['context'][:100]}...\")\n",
        "    print(f\"Question: {example['question']}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(\"-\" * 80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics for GPT-2 QA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_exact_match(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Compute exact match score between prediction and ground truth.\n",
        "    Returns 1 if exact match, 0 otherwise.\n",
        "    \"\"\"\n",
        "    return int(prediction.strip().lower() == ground_truth.strip().lower())\n",
        "\n",
        "def compute_f1(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Compute F1 score between prediction and ground truth.\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    \"\"\"\n",
        "    pred_tokens = prediction.strip().lower().split()\n",
        "    truth_tokens = ground_truth.strip().lower().split()\n",
        "    \n",
        "    # Handle empty predictions/truths\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    \n",
        "    # Find common tokens\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "    \n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # Calculate precision and recall\n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(truth_tokens)\n",
        "    \n",
        "    # Calculate F1 score\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def evaluate_gpt2_qa(model, tokenizer, dataset, max_examples=None, max_new_tokens=50):\n",
        "    \"\"\"\n",
        "    Evaluate GPT-2 QA model using Exact Match and F1 metrics.\n",
        "    Adapted from the original DistilBERT evaluation approach.\n",
        "    \"\"\"\n",
        "    print(\"Starting GPT-2 QA Evaluation...\")\n",
        "    \n",
        "    # Limit examples if specified\n",
        "    examples = dataset if max_examples is None else dataset.select(range(min(max_examples, len(dataset))))\n",
        "    \n",
        "    total_em = 0\n",
        "    total_f1 = 0\n",
        "    total_count = 0\n",
        "    \n",
        "    predictions = []\n",
        "    references = []\n",
        "    \n",
        "    # Process each example\n",
        "    for i, example in enumerate(tqdm(examples, desc=\"Evaluating\")):\n",
        "        question = example['question']\n",
        "        context = example['context']\n",
        "        ground_truths = example['answers']['text']\n",
        "        \n",
        "        # Skip examples with no answers\n",
        "        if not ground_truths:\n",
        "            continue\n",
        "            \n",
        "        # Generate prediction\n",
        "        try:\n",
        "            prediction_text = answer_question_gpt2(question, context, model, tokenizer, max_new_tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer for example {i}: {e}\")\n",
        "            prediction_text = \"\"\n",
        "        \n",
        "        # Store for later analysis\n",
        "        predictions.append({\n",
        "            \"id\": example.get('id', f'example_{i}'),\n",
        "            \"question\": question,\n",
        "            \"context\": context[:200] + \"...\" if len(context) > 200 else context,\n",
        "            \"prediction\": prediction_text,\n",
        "            \"ground_truths\": ground_truths\n",
        "        })\n",
        "        \n",
        "        # Calculate metrics against all possible ground truths\n",
        "        if ground_truths:\n",
        "            em_scores = [compute_exact_match(prediction_text, gt) for gt in ground_truths]\n",
        "            f1_scores = [compute_f1(prediction_text, gt) for gt in ground_truths]\n",
        "            \n",
        "            # Take the best score among all ground truths\n",
        "            best_em = max(em_scores)\n",
        "            best_f1 = max(f1_scores)\n",
        "            \n",
        "            total_em += best_em\n",
        "            total_f1 += best_f1\n",
        "            total_count += 1\n",
        "    \n",
        "    # Calculate final metrics\n",
        "    exact_match = (total_em / total_count) * 100 if total_count > 0 else 0\n",
        "    f1_score = (total_f1 / total_count) * 100 if total_count > 0 else 0\n",
        "    \n",
        "    results = {\n",
        "        \"exact_match\": exact_match,\n",
        "        \"f1_score\": f1_score,\n",
        "        \"total_examples\": total_count,\n",
        "        \"predictions\": predictions\n",
        "    }\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation on Validation Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on a subset of validation data\n",
        "print(\"Evaluating GPT-2 LoRA model on validation set...\")\n",
        "\n",
        "# Evaluate on a smaller subset first (100 examples) for quick testing\n",
        "eval_results = evaluate_gpt2_qa(\n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    dataset=dataset[\"validation\"], \n",
        "    max_examples=100,  # Increase this for full evaluation\n",
        "    max_new_tokens=50\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nEvaluation Results (on {eval_results['total_examples']} examples):\")\n",
        "print(f\"Exact Match: {eval_results['exact_match']:.2f}%\")\n",
        "print(f\"F1 Score: {eval_results['f1_score']:.2f}%\")\n",
        "\n",
        "# Show some example predictions\n",
        "print(f\"\\nSample Predictions:\")\n",
        "print(\"-\" * 100)\n",
        "for i, pred in enumerate(eval_results['predictions'][:5]):  # Show first 5\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Question: {pred['question']}\")\n",
        "    print(f\"Context: {pred['context']}\")\n",
        "    print(f\"Ground Truth: {pred['ground_truths']}\")\n",
        "    print(f\"Prediction: '{pred['prediction']}'\")\n",
        "    \n",
        "    # Calculate individual scores for this example\n",
        "    best_em = max([compute_exact_match(pred['prediction'], gt) for gt in pred['ground_truths']])\n",
        "    best_f1 = max([compute_f1(pred['prediction'], gt) for gt in pred['ground_truths']])\n",
        "    print(f\"EM: {best_em}, F1: {best_f1:.3f}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison with Original Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with original DistilBERT results\n",
        "print(\"Performance Comparison:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Original DistilBERT (from question-answering-with-transformers.ipynb):\")\n",
        "print(\"  - Exact Match: 74.73%\")\n",
        "print(\"  - F1 Score: 84.81%\")\n",
        "print(\"  - Model Type: Extractive QA\")\n",
        "print(\"  - Parameters: ~66M\")\n",
        "print(\"  - Training: Full fine-tuning\")\n",
        "\n",
        "print(f\"\\nGPT-2 LoRA (Current):\")\n",
        "print(f\"  - Exact Match: {eval_results['exact_match']:.2f}%\")\n",
        "print(f\"  - F1 Score: {eval_results['f1_score']:.2f}%\")\n",
        "print(f\"  - Model Type: Generative QA\")\n",
        "print(f\"  - Parameters: ~124M total (~{sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.1f}M trainable)\")\n",
        "print(f\"  - Training: LoRA fine-tuning\")\n",
        "\n",
        "print(f\"\\nKey Differences:\")\n",
        "print(\"• DistilBERT: Extracts exact spans from context\")\n",
        "print(\"• GPT-2: Generates answers based on understanding\")\n",
        "print(\"• LoRA: Only ~0.3% of parameters trained vs 100% for DistilBERT\")\n",
        "print(\"• GPT-2 can potentially provide more natural/creative answers\")\n",
        "\n",
        "# Performance analysis\n",
        "if eval_results['f1_score'] > 0:\n",
        "    f1_ratio = eval_results['f1_score'] / 84.81\n",
        "    em_ratio = eval_results['exact_match'] / 74.73\n",
        "    print(f\"\\nRelative Performance:\")\n",
        "    print(f\"• F1 Score: {f1_ratio:.1%} of original DistilBERT\")\n",
        "    print(f\"• Exact Match: {em_ratio:.1%} of original DistilBERT\")\n",
        "    \n",
        "    if f1_ratio > 0.7:\n",
        "        print(\"• Strong performance considering parameter efficiency!\")\n",
        "    elif f1_ratio > 0.5:\n",
        "        print(\"• Decent performance with much fewer trainable parameters\")\n",
        "    else:\n",
        "        print(\"• May need more training or hyperparameter tuning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Analysis and LoRA Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the LoRA configuration\n",
        "print(\"LoRA Configuration Analysis:\")\n",
        "print(f\"Rank (r): {lora_config.r}\")\n",
        "print(f\"Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"Target modules: {lora_config.target_modules}\")\n",
        "\n",
        "# Print trainable parameters breakdown\n",
        "print(\"\\nTrainable Parameters Breakdown:\")\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "        if 'lora' in name.lower():\n",
        "            print(f\"  {name}: {param.numel():,} parameters\")\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n",
        "\n",
        "# Show memory efficiency\n",
        "print(f\"\\nMemory Efficiency:\")\n",
        "print(f\"Original model would require training: {total_params:,} parameters\")\n",
        "print(f\"With LoRA, only training: {trainable_params:,} parameters\")\n",
        "print(f\"Memory reduction: {100 * (1 - trainable_params / total_params):.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
